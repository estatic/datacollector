<?xml version="1.0" encoding="UTF-8"?>
<!--

    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_xvp_x2x_fz">
 <title>Monitor Dataflow Operations</title>
 <shortdesc>As a DevOps or site reliability engineer, you can monitor your day-to-day operations by
        defining data SLAs (service level agreements) to ensure that incoming data meets business
        requirements for availability and accuracy.</shortdesc>
 <conbody>
  <p>
            <draft-comment author="alisontaylor">text copied from same topic in Control Hub User
                Guide. Make the same updates in both places. </draft-comment>
        </p>
        <p>In addition to measuring the health of a topology, you define data SLAs to define the
            expected thresholds of the data throughput rate or the error record rate. Data SLAs
            trigger an alert when the specified threshold is reached. Data SLA alerts provide
            immediate feedback on the data processing rates expected by your team. They enable you
            to monitor your dataflow operations and quickly investigate and resolve issues that
            arise.</p>
        <p>For example, you have service level agreements with the operational analytics team to
            ensure that all of the data captured and processed in the Customer 360 topology is clean
            and available for immediate analysis. If any of the Customer 360 jobs encounter
            processing errors, you must immediately resolve those issues. You define and activate a
            data SLA that triggers an alert when a job in the topology encounters more than 100
            error records per second. </p>
        <p>If the alert triggers, <ph
                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/DPM-short"/>
            notifies you with a red Notifications icon in the top toolbar: <image
                href="../Graphics/icon_Notifications.png" scale="70" id="image_gk1_rfx_fz"/>. You
            drill into the details of the data SLA to discover which threshold was reached and to
            investigate the issues that need to be resolved. The triggered data SLA displays a graph
            of the error record rate. The red line in the graph represents the defined threshold, as
            follows:</p>
        <p><image href="../Graphics/DPM_MasterOperations.png" scale="55" id="image_q5c_y2c_fbb"
            /></p>
        <p>We've seen how you can use <ph
                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/DPM-short"/> to
            turn a high-level architecture diagram of your dataflows into pipelines and jobs that
            you can then manage and measure from a single topology. Give it a try, and see for
            yourself how easily you can control all of your complex dataflow pipelines with <ph
                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/DPM-short"
            />.</p>
 </conbody>
</concept>
