<?xml version="1.0" encoding="UTF-8"?>
<!--
  Copyright 2017 StreamSets Inc.

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
  
      http://www.apache.org/licenses/LICENSE-2.0
      
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_rr2_mbz_w1b">
 <title>What's New in 2.7.0.0</title>
 <conbody>
  <p><indexterm>what's new<indexterm>version 2.7.0.0</indexterm></indexterm><ph
                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>
            version 2.7.0.0 includes the following new features and enhancements:</p>
        <dl>
            <dlentry>
                <dt>Credential Stores</dt>
                <dd><ph
                        conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                    /> now has a <xref
                        href="../Configuration/CredentialStores.dita#concept_bt1_bpj_r1b">credential
                        store</xref> API that integrates with the following credential store
                        systems:<ul id="ul_vgr_3cz_w1b">
                        <li>Java keystore</li>
                        <li>Hashicorp Vault</li>
                    </ul></dd>
                <dd>
                    <p>You define the credentials required by external systems - user names,
                        passwords, or access keys - in a Java keystore file or in Vault. Then you
                        use <xref
                            href="../Expression_Language/CredentialFunctions.dita#concept_yvc_3qs_r1b"
                            >credential expression language functions</xref> in JDBC stage
                        properties to retrieve those values, instead of directly entering credential
                        values in stage properties. </p>
                </dd>
                <dd>
                    <p>The following JDBC stages can use the new credential functions:<ul
                            id="ul_ezm_4cz_w1b">
                            <li>JDBC Multitable Consumer origin</li>
                            <li>JDBC Query Consumer origin</li>
                            <li>Oracle CDC Client origin</li>
                            <li>SQL Server CDC Client origin</li>
                            <li>SQL Server Change Tracking origin</li>
                            <li>JDBC Lookup processor</li>
                            <li>JDBC Tee processor</li>
                            <li>JDBC Producer destination</li>
                            <li>JDBC Query executor</li>
                        </ul></p>
                </dd>
            </dlentry>
            <dlentry>
                <dt>Publish Pipeline Metadata to Cloudera Navigator (Beta)</dt>
                <dd>
                    <p><ph
                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                        /> now provides beta support for publishing metadata about running pipelines
                        to Cloudera Navigator. You can then use Cloudera Navigator to explore the
                        pipeline metadata, including viewing lineage diagrams of the metadata.</p>
                </dd>
                <dd>Feel free to try out this feature in a development or test <ph
                        conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                    />, and send us your feedback. We are continuing to refine metadata publishing
                    as we gather input from the community and work with Cloudera.</dd>
            </dlentry>
            <dlentry>
                <dt>Stage Libraries</dt>
                <dd>
                    <p>Data Collector includes the following new stage libraries:<ul
                            id="ul_cxp_d2z_w1b">
                            <li>Apache Kudu version 1.4.0</li>
                            <li>Cloudera CDH version 5.12 distribution of Hadoop</li>
                            <li>Cloudera version 5.12 distribution of Apache Kafka 2.1</li>
                            <li>Google Cloud - Includes the Google BigQuery origin, Google Pub/Sub
                                Subscriber origin, and Google Pub/Sub Publisher destination.</li>
                            <li>Java keystore credential store - For use with <xref
                                    href="../Configuration/CredentialStores.dita#concept_bt1_bpj_r1b"
                                    >credential stores</xref>.</li>
                            <li>Vault credential store - For use with <xref
                                    href="../Configuration/CredentialStores.dita#concept_bt1_bpj_r1b"
                                    >credential stores</xref>.</li>
                        </ul></p>
                </dd>
            </dlentry>
            <dlentry>
                <dt>Data Collector Configuration</dt>
                <dd>
                    <ul id="ul_hc5_q2z_w1b">
                        <li><xref href="../Configuration/Vault-Overview.dita#concept_bmq_gl1_mw"
                                >Access  Hashicorp Vault secrets</xref> - The Data Collector Vault
                            integration now relies on Vault's App Role authentication backend.
                            Previously, Data Collector relied on Vault's App ID authentication
                            backend. Hashicorp has deprecated the App ID authentication
                            backend.</li>
                        <li><xref
                                href="../Configuration/HadoopImpersonationMode-Lowercase.dita#concept_dcq_mpk_f1b"
                                >New Hadoop user impersonation property</xref> - When you enable
                            Data Collector to impersonate the current Data Collector user when
                            writing to Hadoop, you can now also configure Data Collector to make the
                            username lowercase. This can be helpful with case-sensitive
                            implementations of LDAP.</li>
                        <li><xref
                                href="../Configuration/ConfiguringDataCollector.dita#task_lxk_kjw_1r"
                                >New Java security properties</xref> - The Data Collector
                            configuration file now includes properties with a "java.security."
                            prefix, which you can use to configure Java security properties.</li>
                        <li><xref
                                href="../Configuration/ConfiguringDataCollector.dita#task_lxk_kjw_1r"
                                >New property to define the amount of time to cache DNS
                                lookups</xref> - By default, the
                            java.security.networkaddress.cache.ttl property is set to 0 so that the
                            JVM uses the Domain Name Service (DNS) time to live value, instead of
                            caching the lookups for the lifetime of the JVM.</li>
                        <li><xref href="../Configuration/HeapDumpCreation.dita#concept_cy3_r44_z1b"
                                >SDC_HEAPDUMP_PATH enhancement</xref> - The new default file name,
                                <codeph>$SDC_LOG/sdc_heapdump_${timestamp}.hprof</codeph>, includes
                            a timestamp so you can write multiple heap dump files to the specified
                            directory.</li>
                    </ul>
                </dd>
            </dlentry>
            <dlentry>
                <dt>Dataflow Triggers</dt>
                <dd>
                    <ul id="ul_fg2_gfz_w1b">
                        <li><xref href="../Event_Handling/PipelineEvents.dita#concept_amg_2qr_t1b"
                                >Pipeline events</xref> - The event framework now generates pipeline
                            lifecycle events when the pipeline stops and starts. You can pass each
                            pipeline event to an executor or to another pipeline for more complex
                            processing. Use pipeline events to trigger tasks before pipeline
                            processing begins or after it stops.</li>
                    </ul>
                </dd>
            </dlentry>
            <dlentry>
                <dt>Origins</dt>
                <dd>
                    <ul id="ul_tpp_sfz_w1b">
                        <li><xref href="../Origins/BigQuery.dita#concept_cg3_y3v_q1b">New Google
                                BigQuery origin</xref> - An origin that executes a query job and
                            reads the result from Google BigQuery.</li>
                        <li><xref href="../Origins/PubSub.dita#concept_pjw_qtl_r1b">New Google
                                Pub/Sub Subscriber origin</xref> - A multithreaded origin that
                            consumes messages from a Google Pub/Sub subscription.</li>
                        <li><xref href="../Origins/OPCUAClient.dita#concept_nmf_1ly_f1b">New OPC UA
                                Client origin</xref> - An origin that processes data from an OPC UA
                            server.</li>
                        <li><xref href="../Origins/SQLServerCDC.dita#concept_ut3_ywc_v1b">New SQL
                                Server CDC Client origin</xref> - A multithreaded origin that reads
                            data from Microsoft SQL Server CDC tables. </li>
                        <li><xref href="../Origins/SQLServerChange.dita#concept_ewq_b2s_r1b">New SQL
                                Server Change Tracking origin</xref> - A multithreaded origin that
                            reads data from Microsoft SQL Server change tracking tables and
                            generates the latest version of each record.</li>
                        <li><xref
                                href="../Origins/Directory-EventGeneration.dita#concept_ttg_vgn_qx"
                                >Directory origin event enhancements</xref> - The Directory origin
                            can now generate no-more-data events when it completes processing all
                            available files and the batch wait time has elapsed without the arrival
                            of new files. Also, the File Finished event now includes the number of
                            records and files processed. </li>
                        <li><xref href="../Origins/Hadoop-OtherFS.dita#concept_ogc_xzd_f1b">Hadoop
                                FS origin enhancement</xref> - The Hadoop FS origin now allows you
                            to read data from other file systems using the Hadoop FileSystem
                            interface. Use the Hadoop FS origin in cluster batch pipelines.</li>
                        <li><xref href="../Origins/HTTPClient-Configuring.dita#task_akl_rkz_5r">HTTP
                                Client origin enhancement</xref> - The HTTP Client origin now allows
                            time functions and datetime variables in the request body. It also
                            allows you to specify the time zone to use when evaluating the request
                            body. </li>
                        <li><xref href="../Origins/HTTPServer-DataFormat.dita#concept_anf_ss4_qy"
                                >HTTP Server origin enhancement</xref> - The HTTP Server origin can
                            now process Avro files.</li>
                        <li><xref href="../Origins/JDBCConsumer-Configuring.dita#task_ryz_tkr_bs"
                                >JDBC Query Consumer origin enhancement</xref> - You can now
                            configure the behavior for the origin when it encounters data of an
                            unknown data type.</li>
                        <li><xref href="../Origins/MultiTableJDBCConsumer.dita#concept_zp3_wnw_4y"
                                >JDBC Multitable Consumer origin enhancements</xref>:<ul
                                id="ul_gnl_rhz_w1b">
                                <li>You can now use the origin to perform multithreaded processing
                                    of partitions within a table. Use partition processing to handle
                                    even larger volumes of data. This enhancement also includes new
                                    JDBC header attributes.<p>By default, all new pipelines use
                                        partition processing when possible. Upgraded pipelines use
                                        multithreaded table processing to preserve previous
                                        behavior.</p></li>
                                <li>You  can now configure the behavior for the origin when it
                                    encounters data of an unknown data type.</li>
                            </ul></li>
                        <li><xref href="../Origins/OracleCDC.dita#concept_rs5_hjj_tw">Oracle CDC
                                Client origin enhancements</xref>:<ul id="ul_ych_yhz_w1b">
                                <li>The origin can now buffer data locally rather than utilizing
                                    Oracle LogMiner buffers.</li>
                                <li>You can now specify the behavior when the origin encounters an
                                    unsupported field type - send to the pipeline, send to error, or
                                    discard.</li>
                                <li>You can configure the origin to include null values passed from
                                    the LogMiner full supplemental logging. By default, the origin
                                    ignores null values.</li>
                                <li>You now must select the target server time zone for the origin. </li>
                                <li>You can now configure a query timeout for the origin.</li>
                                <li>The origin now includes the row ID in the oracle.cdc.rowId
                                    record header attribute and can include the LogMiner redo query
                                    in the oracle.cdc.query record header attribute.</li>
                            </ul></li>
                        <li><xref
                                href="../Origins/RabbitMQ-HeaderAttributes.dita#concept_rg5_yts_y1b"
                                >RabbitMQ Consumer origin enhancement</xref> - When available, the
                            origin now provides attributes generated by RabbitMQ, such as
                            contentType, contentEncoding, and deliveryMode, as record header
                            attributes.</li>
                        <li><xref href="../Origins/TCPServer-TCPMode.dita#concept_bqt_tl4_sz">TCP
                                Server origin enhancement</xref> - The origin can now process
                            character-based data that includes a length prefix. </li>
                        <li><xref href="../Origins/UDP-RawData.dita#concept_jhh_ryx_r1b">UDP Source
                                origin enhancement</xref> - The origin can now process binary and
                            character-based raw data.</li>
                        <li>New last-modified time record header attribute - <xref
                                href="../Origins/Directory-RecordHeaderAtts.dita#concept_tlj_3g1_2z"
                                >Directory</xref>, <xref
                                href="../Origins/FileTail-RecordHeaderAtts.dita#concept_tlj_3g1_2z"
                                >File Tail</xref>, and <xref
                                href="../Origins/SFTP-RecordHeaderAtts.dita#concept_tlj_3g1_2z"
                                >SFTP/FTP Client</xref> origins now include the last modified time
                            for the originating file for a record in an mtime record header
                            attribute.</li>
                    </ul>
                </dd>
            </dlentry>
            <dlentry>
                <dt>Processors</dt>
                <dd>
                    <ul id="ul_py4_vmz_w1b">
                        <li><xref href="../Processors/DataParser.dita#concept_xw3_4xk_r1b">New Data
                                Parser processor</xref> - Use the new processor to extract NetFlow
                            or syslog messages as well as other supported data formats that are
                            embedded in a field. </li>
                        <li><xref href="../Processors/JSONParser.dita#concept_bs1_4t3_yq">New JSON
                                Generator processor</xref> - Use the new processor to serialize data
                            from a record field to a JSON-encoded string.</li>
                        <li><xref href="../Processors/KuduLookup.dita#concept_a1x_3wl_p1b">New Kudu
                                Lookup processor</xref> - Use the new processor to perform lookups
                            in Kudu to enrich records with additional data.</li>
                        <li><xref href="../Processors/HiveMeta-CustomAtts.dita#concept_jv2_jjn_l1b"
                                >Hive Metadata processor enhancement</xref> - You can now configure
                            custom record header attributes for metadata records.</li>
                    </ul>
                </dd>
            </dlentry>
            <dlentry>
                <dt>Destinations</dt>
                <dd>
                    <ul id="ul_wdl_lnz_w1b">
                        <li><xref href="../Destinations/PubSubPublisher.dita#concept_qsj_hk1_v1b"
                                >New Google Pub/Sub Publisher destination</xref> - A destination
                            that publishes messages to Google Pub/Sub.</li>
                        <li><xref href="../Destinations/JMSProducer.dita#concept_sfz_ww5_n1b">New
                                JMS Producer destination</xref> - A destination that writes data to
                            JMS.</li>
                        <li>Amazon S3 destination enhancements:<ul id="ul_ls3_d4z_w1b">
                                <li>You can now use expressions in the <xref
                                        href="../Destinations/AmazonS3-Bucket.dita#concept_bnp_gwp_f1b"
                                        >Bucket property</xref> for the Amazon S3 destination. This
                                    enables you to write records dynamically based expression
                                    evaluation.</li>
                                <li>The Amazon S3 object written <xref
                                        href="../Destinations/AmazonS3-EventRecords.dita#concept_nly_sw2_px"
                                        >event record</xref> now includes the number of records
                                    written to the object.</li>
                            </ul></li>
                        <li><xref
                                href="../Destinations/DataLakeStore-Configuring.dita#task_jfl_nf4_zx"
                                >Azure Data Lake Store destination enhancement</xref> - The Client
                            ID and Client Key properties have been renamed Application ID and
                            Application Key to align with the updated property names in the new
                            Azure portal.</li>
                        <li><xref
                                href="../Destinations/Cassandra-Authentication.dita#concept_ajh_vhp_x1b"
                                >Cassandra destination enhancement</xref> - The destination now
                            supports Kerberos authentication if you have installed the DataStax
                            Enterprise Java driver. </li>
                        <li><xref
                                href="../Destinations/Elasticsearch-Configuring.dita#task_uns_gtv_4r"
                                >Elasticsearch destination enhancement</xref> - The destination can
                            now create parent-child relationships between documents in the same
                            index.</li>
                        <li><xref href="../Destinations/HiveMetastore.dita#concept_gcr_z2t_zv">Hive
                                Metastore destination</xref> - You can now configure the destination
                            to create custom record header attributes.</li>
                        <li><xref
                                href="../Destinations/KProducer-DataFormat.dita#concept_lww_3b3_kr"
                                >Kafka Producer destination enhancement</xref> - The destination can
                            now write XML documents.</li>
                        <li><xref href="../Destinations/Solr-configuring.dita#task_ld1_phr_wr">Solr
                                destination enhancement</xref> - You can now configure the
                            destination to skip connection validation when the Solr configuration
                            file, <codeph>solrconfig.xml</codeph>, does not define the default
                            search field (“df”) parameter.</li>
                    </ul>
                </dd>
            </dlentry>
            <dlentry>
                <dt>Executors</dt>
                <dd>
                    <ul id="ul_gqx_fpz_w1b">
                        <li><xref href="../Executors/AmazonS3.dita#concept_mvh_bnm_f1b">New Amazon
                                S3 executor</xref> - Use the Amazon S3 executor to create new Amazon
                            S3 objects for the specified content or add tags to existing objects
                            each time it receives an event.</li>
                        <li><xref
                                href="../Executors/HDFSFileMeta-RemovingFileDir.dita#concept_yf2_hc4_x1b"
                                >HDFS File Metadata executor enhancement</xref> - The executor can
                            now remove a file or directory when it receives an event.</li>
                    </ul>
                </dd>
            </dlentry>
            <dlentry>
                <dt>Dataflow Performance Manager</dt>
                <dd>
                    <ul id="ul_dwx_lpz_w1b">
                        <li><xref href="../DPM/RevertPublishedPipelines.dita#task_c4x_vff_p1b"
                                >Revert changes to published pipelines</xref> - If you update a
                            published pipeline but decide not to publish the updates to DPM as a new
                            version, you can revert the changes made to the pipeline
                            configuration.</li>
                    </ul>
                </dd>
            </dlentry>
            <dlentry>
                <dt>Pipelines</dt>
                <dd>
                    <ul id="ul_ngb_qpz_w1b">
                        <li><xref href="../Pipeline_Design/ErrorHandling.dita#concept_pm4_txm_vq"
                                >Pipeline error handling enhancements</xref>: <ul
                                id="ul_rq1_tpz_w1b">
                                <li>Use the new Error Record Policy to specify the version of the
                                    record to include in error records. </li>
                                <li>You can now write error records to Amazon Kinesis Streams.</li>
                            </ul></li>
                        <li><xref
                                href="../Pipeline_Design/InternalAttributes.dita#concept_itf_55z_dz"
                                >Error records enhancement</xref> - Error records now include the
                            user-defined stage label in the errorStageLabel header attribute.</li>
                        <li><xref
                                href="../Pipeline_Maintenance/PipelineStates-Understanding.dita#concept_s4p_ns5_nz"
                                >Pipeline state enhancements</xref> - Pipelines can now display the
                            following new states: STARTING_ERROR, STOPPING_ERROR, and
                            STOP_ERROR.</li>
                    </ul>
                </dd>
            </dlentry>
            <dlentry>
                <dt>Data Formats</dt>
                <dd>
                    <ul id="ul_btq_yqz_w1b">
                        <li><xref href="../Data_Formats/WritingXML.dita#concept_t2m_hhx_41b">Writing
                                XML</xref> - You can now use the Google Pub/Sub Publisher, JMS
                            Producer, and Kafka Producer destinations to write XML documents to
                            destination systems. Note the record structure requirement before you
                            use this data format. </li>
                        <li>Avro:<ul id="ul_k4b_yrz_w1b">
                                <li>Origins now write the Avro schema to an avroSchema record header
                                    attribute.</li>
                                <li>Origins now include precision and scale <xref
                                        href="../Pipeline_Design/FieldAttributes.dita#concept_xfm_wtp_1z"
                                        >field attributes</xref> for every Decimal field.</li>
                                <li>Data Collector now supports the time-based logical types added
                                    to Avro in version 1.8.</li>
                            </ul></li>
                        <li>Delimited - Data Collector can now continue processing records with
                            delimited data when a row has more fields than the header. Previously,
                            rows with more fields than the header were sent to error. </li>
                    </ul>
                </dd>
            </dlentry>
            <dlentry>
                <dt>Cluster Pipelines</dt>
                <dd>
                    <p>This release includes the following <xref
                            href="../Cluster_Mode/ClusterPipelines-ExecutionMode.dita#concept_rjc_4m5_lx"
                            >Cluster Yarn Streaming enhancements</xref>:<ul id="ul_b14_nsz_w1b">
                            <li>Use a new Worker Count property to limit the number of worker nodes
                                used in Cluster Yarn Streaming pipelines. By default, a Data
                                Collector worker is spawned for each partition in the topic. </li>
                            <li>You can now define Spark configuration properties to pass to the
                                spark-submit script.</li>
                        </ul></p>
                </dd>
            </dlentry>
            <dlentry>
                <dt>Expression Language</dt>
                <dd>
                    <p>This release includes the following new functions:<ul id="ul_abf_qsz_w1b">
                            <li><xref
                                    href="../Expression_Language/CredentialFunctions.dita#concept_yvc_3qs_r1b"
                                    >credential:get()</xref> - Returns credential values from a
                                credential store.</li>
                            <li><xref
                                    href="../Expression_Language/CredentialFunctions.dita#concept_yvc_3qs_r1b"
                                    >credential:getWithOptions()</xref> - Returns credential values
                                from a credential store using additional options to communicate with
                                the credential store.</li>
                            <li><xref
                                    href="../Expression_Language/ErrorFunctions.dita#concept_ndj_43v_1r"
                                    >record:errorStageLabel()</xref> - Returns the user-defined name
                                of the stage that generated the error record.</li>
                            <li><xref
                                    href="../Expression_Language/MiscFunctions.dita#concept_ddw_ld1_1s"
                                    >list:join()</xref>  - Merges elements in a List field into a
                                String field, using the specified separator between elements. </li>
                            <li><xref
                                    href="../Expression_Language/MiscFunctions.dita#concept_ddw_ld1_1s"
                                    >list:joinSkipNulls()</xref> - Merges elements in a List field
                                into a String field, using the specified separator between elements
                                and skipping null values.</li>
                        </ul></p>
                </dd>
                <dd>
                    <p>
                        <ul id="ul_k5c_rg4_z1b">
                            <li><xref
                                    href="../Expression_Language/StringFunctions.dita#concept_ahp_f4v_1r"
                                    >str:indexOf()</xref> - Returns the index within a string of the
                                first occurrence of the specified subset of characters.</li>
                        </ul>
                    </p>
                </dd>
            </dlentry>
            <dlentry>
                <dt>Miscellaneous</dt>
                <dd>
                    <ul id="ul_y2k_htz_w1b">
                        <li><xref
                                href="../Pipeline_Configuration/SimpleBulkEdit.dita#concept_alb_b3y_cbb"
                                >Global bulk edit mode</xref> - In any property where you would
                            previously click an Add icon to add additional configurations, you can
                            now switch to bulk edit mode to enter a list of configurations in JSON
                            format.</li>
                        <li>Snapshot enhancement - Snapshots no longer produce empty batches when
                            waiting for data.</li>
                        <li><xref
                                href="../Pipeline_Configuration/Webhooks-Payload_Parameters.dita#concept_rby_1rl_rz"
                                >Webhooks enhancement</xref> - You can use several new pipeline
                            state notification parameters in webhooks.</li>
                    </ul>
                </dd>
            </dlentry>
        </dl>
 </conbody>
</concept>
